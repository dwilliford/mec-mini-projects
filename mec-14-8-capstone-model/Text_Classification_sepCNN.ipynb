{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhAEgCTZzjZe"
   },
   "source": [
    "This colab notebook is based on : https://developers.google.com/machine-learning/guides/text-classification.\n",
    "\n",
    "Note this is the Option B implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFjkX2W616GR"
   },
   "source": [
    "Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24599,
     "status": "ok",
     "timestamp": 1643689575343,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "bNEWZPvyyX2O",
    "outputId": "1677bf55-f1cd-4174-8bed-d43f88b7b2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# the base Google Drive directory\n",
    "root_dir = \"/content/drive/My Drive/\"\n",
    "\n",
    "# Should probably organize by project\n",
    "#project_folder = \"Colab Notebooks/My Project Folder/\"\n",
    "\n",
    "base_data_location = root_dir + 'Colab Notebooks/data'\n",
    "devotional_corpus = base_data_location + '/corpus_mod.json'\n",
    "\n",
    "#load devotionals\n",
    "df = pd.read_json(devotional_corpus)\n",
    "\n",
    "# Get devotional ids for each collection and use as a label for supervised training\n",
    "#collections = {\"782\" : \"love\", \"924\" : \"joy\", \"290\" : \"peace\", \"906\" : \"hope\", \"809\" : \"depression\"}\n",
    "# Will also encode here\n",
    "collections = {\"782\" : 0, \"924\" : 1, \"290\" : 2, \"906\" : 3, \"809\" : 4}\n",
    "#collections = {\"1\" : \"toy\"}\n",
    "devo_labels = {}\n",
    "for collection in collections.keys():\n",
    "    input_file = open(base_data_location + '/collections/collection_' + str(collection) + '.json')\n",
    "    collection_data = json.load(input_file)\n",
    "    page = {}\n",
    "    for page in collection_data.values():\n",
    "        for reading_plan in page['collections'][0]['items']:\n",
    "            reading_plan_id = reading_plan['id']\n",
    "            #create array with reading_plan id and collection id\n",
    "            devo_labels[reading_plan_id] = collections[collection]\n",
    "    input_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5218,
     "status": "ok",
     "timestamp": 1643689580556,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "GWfmmTFSDibJ",
    "outputId": "63bda622-459b-460c-aea8-cf84817884f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  import nltk\n",
    "  nltk.download('stopwords')\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1643689580557,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "rcQV64Hw4D20"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def clean(text):\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lower = [word.lower() for word in tokens]\n",
    "    no_stopwords = [word for word in lower if word not in stopword]\n",
    "    no_alpha = [word for word in no_stopwords if word.isalpha()]\n",
    "    lemm_text = [wn.lemmatize(word) for word in no_alpha]\n",
    "    clean_text = lemm_text\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1643689580813,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "l7rlfKm1FWBW",
    "outputId": "ba14a603-7f2a-42f9-972d-acf333565316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                source_id  ...                                         references\n",
      "YV_RP_29045_1      29045  ...  [JHN.3.16+JHN.3.17, LUK.2.11, ISA.7.14, COL.1....\n",
      "YV_RP_29045_2      29045  ...  [EPH.2.8+EPH.2.9, 2CO.5.21, EPH.1.7, ROM.6.6, ...\n",
      "YV_RP_29045_3      29045  ...                     [JHN.14.6, JHN.11.25, ACT.1.8]\n",
      "YV_RP_28889_1      28889  ...                                         [JAS.2.14]\n",
      "YV_RP_28889_2      28889  ...                                        [PRO.12.18]\n",
      "...                  ...  ...                                                ...\n",
      "YV_RP_28716_1      28716  ...  [GEN.1.28, GEN.1.31, PSA.116.7, LUK.10.30+LUK....\n",
      "YV_RP_28716_2      28716  ...                                  [GEN.5.1+GEN.5.2]\n",
      "YV_RP_28716_3      28716  ...  [GEN.1.26, EXO.20.4+EXO.20.5+EXO.20.6, DEU.5.8...\n",
      "YV_RP_28716_4      28716  ...  [GEN.1.28, GEN.14.18+GEN.14.19+GEN.14.20, PSA....\n",
      "YV_RP_28716_5      28716  ...  [PSA.110.1, ISA.53, MRK.10.45, HEB.2.14+HEB.2.15]\n",
      "\n",
      "[7574 rows x 6 columns]>\n",
      "source_id                                                 29045\n",
      "source                                               YouVersion\n",
      "type                                               reading plan\n",
      "day                                                           1\n",
      "text          [IMAGE CONTENT] \\n\\nTHE GIFT OF JESUS \\n\\n  \\n...\n",
      "references    [JHN.3.16+JHN.3.17, LUK.2.11, ISA.7.14, COL.1....\n",
      "Name: YV_RP_29045_1, dtype: object\n",
      "source_id                                                 28850\n",
      "source                                               YouVersion\n",
      "type                                               reading plan\n",
      "day                                                           4\n",
      "text          Full of Joy \\n\\nWhat’s the secret to full joy?...\n",
      "references                                          [JHN.16.24]\n",
      "Name: YV_RP_28850_4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.head)\n",
    "#print(df.iloc[0:2,])\n",
    "print(df.iloc[0])\n",
    "print(df.iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40106,
     "status": "ok",
     "timestamp": 1643689626736,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "XQGc5Z3t2nNF",
    "outputId": "2fc22b12-8ba7-4fb4-8faf-320142df2a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                source_id  ...                                         references\n",
      "YV_RP_29045_1      29045  ...  [JHN.3.16+JHN.3.17, LUK.2.11, ISA.7.14, COL.1....\n",
      "YV_RP_29045_2      29045  ...  [EPH.2.8+EPH.2.9, 2CO.5.21, EPH.1.7, ROM.6.6, ...\n",
      "YV_RP_29045_3      29045  ...                     [JHN.14.6, JHN.11.25, ACT.1.8]\n",
      "YV_RP_28889_1      28889  ...                                         [JAS.2.14]\n",
      "YV_RP_28889_2      28889  ...                                        [PRO.12.18]\n",
      "...                  ...  ...                                                ...\n",
      "YV_RP_28716_1      28716  ...  [GEN.1.28, GEN.1.31, PSA.116.7, LUK.10.30+LUK....\n",
      "YV_RP_28716_2      28716  ...                                  [GEN.5.1+GEN.5.2]\n",
      "YV_RP_28716_3      28716  ...  [GEN.1.26, EXO.20.4+EXO.20.5+EXO.20.6, DEU.5.8...\n",
      "YV_RP_28716_4      28716  ...  [GEN.1.28, GEN.14.18+GEN.14.19+GEN.14.20, PSA....\n",
      "YV_RP_28716_5      28716  ...  [PSA.110.1, ISA.53, MRK.10.45, HEB.2.14+HEB.2.15]\n",
      "\n",
      "[7574 rows x 6 columns]>\n",
      "               source_id  ...                                         references\n",
      "YV_RP_29045_1      29045  ...  [JHN.3.16+JHN.3.17, LUK.2.11, ISA.7.14, COL.1....\n",
      "\n",
      "[1 rows x 6 columns]\n",
      "<bound method NDFrame.head of        Collection\n",
      "29045           0\n",
      "28889           0\n",
      "28850           1\n",
      "28924           3\n",
      "28967           0\n",
      "...           ...\n",
      "25185           4\n",
      "16828           4\n",
      "12822           4\n",
      "17577           4\n",
      "28716           4\n",
      "\n",
      "[940 rows x 1 columns]>\n",
      "<bound method NDFrame.head of                source_id  ... Collection\n",
      "YV_RP_29045_1      29045  ...          0\n",
      "YV_RP_29045_2      29045  ...          0\n",
      "YV_RP_29045_3      29045  ...          0\n",
      "YV_RP_28889_1      28889  ...          0\n",
      "YV_RP_28889_2      28889  ...          0\n",
      "...                  ...  ...        ...\n",
      "YV_RP_28716_1      28716  ...          4\n",
      "YV_RP_28716_2      28716  ...          4\n",
      "YV_RP_28716_3      28716  ...          4\n",
      "YV_RP_28716_4      28716  ...          4\n",
      "YV_RP_28716_5      28716  ...          4\n",
      "\n",
      "[7574 rows x 7 columns]>\n",
      "source_id                                                 29045\n",
      "source                                               YouVersion\n",
      "type                                               reading plan\n",
      "day                                                           1\n",
      "text          [IMAGE CONTENT] \\n\\nTHE GIFT OF JESUS \\n\\n  \\n...\n",
      "references    [JHN.3.16+JHN.3.17, LUK.2.11, ISA.7.14, COL.1....\n",
      "Collection                                                    0\n",
      "Name: YV_RP_29045_1, dtype: object\n",
      "source_id                                                 28850\n",
      "source                                               YouVersion\n",
      "type                                               reading plan\n",
      "day                                                           4\n",
      "text          Full of Joy \\n\\nWhat’s the secret to full joy?...\n",
      "references                                          [JHN.16.24]\n",
      "Collection                                                    1\n",
      "Name: YV_RP_28850_4, dtype: object\n",
      "<bound method NDFrame.head of                source_id  ...                                clean_text_combined\n",
      "YV_RP_29045_1      29045  ...  image content gift jesus looking perfect gift ...\n",
      "YV_RP_29045_2      29045  ...  image content gift salvation love getting free...\n",
      "YV_RP_29045_3      29045  ...  share jesus perfect gift christian called witn...\n",
      "YV_RP_28889_1      28889  ...  one another evidence faith meditate james audi...\n",
      "YV_RP_28889_2      28889  ...  word bring healing word bring hurt healing med...\n",
      "...                  ...  ...                                                ...\n",
      "YV_RP_28716_1      28716  ...  beloved beloved word hide within definition lo...\n",
      "YV_RP_28716_2      28716  ...  needy u recall called name want embrace stupid...\n",
      "YV_RP_28716_3      28716  ...  god living statue mean image bearer god phrase...\n",
      "YV_RP_28716_4      28716  ...  name giver every expectant hopeful parent know...\n",
      "YV_RP_28716_5      28716  ...  name every jesus even though name powerful one...\n",
      "\n",
      "[7574 rows x 9 columns]>\n",
      "source_id                                                          29045\n",
      "source                                                        YouVersion\n",
      "type                                                        reading plan\n",
      "day                                                                    1\n",
      "text                   [IMAGE CONTENT] \\n\\nTHE GIFT OF JESUS \\n\\n  \\n...\n",
      "references             [JHN.3.16+JHN.3.17, LUK.2.11, ISA.7.14, COL.1....\n",
      "Collection                                                             0\n",
      "clean_text             [image, content, gift, jesus, looking, perfect...\n",
      "clean_text_combined    image content gift jesus looking perfect gift ...\n",
      "Name: YV_RP_29045_1, dtype: object\n",
      "source_id                                                          28850\n",
      "source                                                        YouVersion\n",
      "type                                                        reading plan\n",
      "day                                                                    4\n",
      "text                   Full of Joy \\n\\nWhat’s the secret to full joy?...\n",
      "references                                                   [JHN.16.24]\n",
      "Collection                                                             1\n",
      "clean_text             [full, joy, secret, full, joy, meditate, john,...\n",
      "clean_text_combined    full joy secret full joy meditate john audio a...\n",
      "Name: YV_RP_28850_4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.head)\n",
    "print(df[0:1])\n",
    "\n",
    "# Create dataframe from label dictionary\n",
    "label_df = pd.DataFrame.from_dict(devo_labels, orient='index', columns=['Collection'])\n",
    "print(label_df.head)\n",
    "\n",
    "# Combine devotionals with collection labels\n",
    "combined_df = pd.merge(df, label_df, how='left', left_on=['source_id'], right_index=True)\n",
    "print(combined_df.head)\n",
    "print(combined_df.iloc[0])\n",
    "print(combined_df.iloc[10])\n",
    "\n",
    "combined_df['clean_text'] = combined_df.apply(lambda row : clean(row['text']), axis = 1)\n",
    "# Need a version with tokens recombined so they can be retokenized later...\n",
    "combined_df['clean_text_combined'] = combined_df.apply(lambda row : \" \".join(row['clean_text']), axis = 1)\n",
    "\n",
    "print(combined_df.head)\n",
    "print(combined_df.iloc[0])\n",
    "print(combined_df.iloc[10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1643689656032,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "KDwvUVEflU-Z"
   },
   "outputs": [],
   "source": [
    "#Create single arrays of cleaned text and labels for training/testing\n",
    "y = combined_df['Collection'].to_numpy()\n",
    "X = combined_df['clean_text_combined'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1643689659382,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "9EmB-ZeA4iey"
   },
   "outputs": [],
   "source": [
    "#Split training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1ZOQ68AzBgS"
   },
   "source": [
    "Step 3: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2212,
     "status": "ok",
     "timestamp": 1643689666196,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "QiF6jRaHyvtc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8sG8OO26x7J"
   },
   "source": [
    "Construct a four-layer sepCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1643689860428,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "zT_8T8P_jM4Z"
   },
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1643690622744,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "9i8FN79w6ziJ"
   },
   "outputs": [],
   "source": [
    "#from tensorflow.python.keras import models\n",
    "#from tensorflow.python.keras import initializers\n",
    "#from tensorflow.python.keras import regularizers\n",
    "\n",
    "#from tensorflow.python.keras.layers import Dense\n",
    "#from tensorflow.python.keras.layers import Dropout\n",
    "#from tensorflow.python.keras.layers import Embedding\n",
    "#from tensorflow.python.keras.layers import SeparableConv1D\n",
    "#from tensorflow.python.keras.layers import MaxPooling1D\n",
    "#from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import SeparableConv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F68CwZ8Oy5SW"
   },
   "source": [
    "Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1643690634009,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "MLspIYr7hVNQ"
   },
   "outputs": [],
   "source": [
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1643690636230,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "hswdK1cxR1YT"
   },
   "outputs": [],
   "source": [
    "\"\"\"Module to train sequence model.\n",
    "\n",
    "Vectorizes training and validation texts into sequences and uses that for\n",
    "training a sequence model - a sepCNN model. We use sequence model for text\n",
    "classification when the ratio of number of samples to number of words per\n",
    "sample for the given dataset is very large (>~15K).\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#import build_model\n",
    "#import load_data\n",
    "#import vectorize_data\n",
    "#import explore_data\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def train_sequence_model(data,\n",
    "                         learning_rate=1e-3,\n",
    "                         epochs=1000,\n",
    "                         batch_size=128,\n",
    "                         blocks=2,\n",
    "                         filters=64,\n",
    "                         dropout_rate=0.2,\n",
    "                         embedding_dim=200,\n",
    "                         kernel_size=3,\n",
    "                         pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    #num_classes = explore_data.get_num_classes(train_labels)\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    #x_train, x_val, word_index = vectorize_data.sequence_vectorize(train_texts, val_texts)\n",
    "    x_train, x_val, word_index = sequence_vectorize(train_texts, val_texts)\n",
    "    \n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    #model = build_model.sepcnn_model(blocks=blocks,\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('devo_classifier_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 71972,
     "status": "ok",
     "timestamp": 1643690920861,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "oPp4FHSEgIZ6",
    "outputId": "cb51b88f-756f-4166-890d-3e566372994c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "42/42 - 24s - loss: 1.5985 - acc: 0.2511 - val_loss: 1.5987 - val_acc: 0.2499 - 24s/epoch - 567ms/step\n",
      "Epoch 2/1000\n",
      "42/42 - 23s - loss: 1.5929 - acc: 0.2615 - val_loss: 1.5993 - val_acc: 0.2499 - 23s/epoch - 548ms/step\n",
      "Epoch 3/1000\n",
      "42/42 - 23s - loss: 1.5931 - acc: 0.2615 - val_loss: 1.5990 - val_acc: 0.2499 - 23s/epoch - 556ms/step\n",
      "Validation accuracy: 0.2498900145292282, loss: 1.5990124940872192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2498900145292282, 1.5990124940872192)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(train_texts, train_labels), (val_texts, val_labels)]\n",
    "train_sequence_model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1643690104187,
     "user": {
      "displayName": "Dave Williford",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1oB0_dFMci-8J9hCWjfFgiRe6DHGeTdAG7OWf2A=s64",
      "userId": "07870752706515771222"
     },
     "user_tz": 480
    },
    "id": "Q2ghE3twkGl-",
    "outputId": "900c3bc8-22b6-4680-ddc9-8c220d23cfb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NEwrmPSkJMh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2hra6BbbBEn3dnOEdccWR",
   "collapsed_sections": [],
   "name": "Text_Classification_sepCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
