{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec927f08",
   "metadata": {},
   "source": [
    "Follow along from : https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145c0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName('mySparkApp')\n",
    "conf.setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c16eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2da3099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = range(1,1000)\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect()\n",
    "rdd.take(2) # It will print first 2 elements of rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421e3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['Hello' , 'I' , 'AM', 'Ankit ', 'Gupta']\n",
    "Rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c0b594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 1), ('I', 1), ('AM', 1), ('Ankit ', 1), ('Gupta', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd1 = Rdd.map(lambda x: (x,1))\n",
    "Rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce1d7a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dave\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# I think I can just use the SparkSession created above instead of SQLContext\n",
    "#from pyspark import SQLContext\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "#C:\\Users\\Dave\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
    "#  warnings.warn("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0308c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = sqlContext.load(source=\"com.databricks.spark.csv\", path = 'PATH/train.csv', header = True,inferSchema = True)\n",
    "#test = sqlContext.load(source=\"com.databricks.spark.csv\", path = 'PATH/test-comb.csv', header = True,inferSchema = True)\n",
    "train = spark.read.csv('data/train.csv', header = True,inferSchema = True)\n",
    "test = spark.read.csv('data/test.csv', header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d680752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d17f0a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(User_ID=1000001, Product_ID='P00069042', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
       " Row(User_ID=1000001, Product_ID='P00248942', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200),\n",
       " Row(User_ID=1000001, Product_ID='P00087842', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n",
       " Row(User_ID=1000001, Product_ID='P00085442', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14, Product_Category_3=None, Purchase=1057),\n",
       " Row(User_ID=1000002, Product_ID='P00285442', Gender='M', Age='55+', Occupation=16, City_Category='C', Stay_In_Current_City_Years='4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969),\n",
       " Row(User_ID=1000003, Product_ID='P00193542', Gender='M', Age='26-35', Occupation=15, City_Category='A', Stay_In_Current_City_Years='3', Marital_Status=0, Product_Category_1=1, Product_Category_2=2, Product_Category_3=None, Purchase=15227),\n",
       " Row(User_ID=1000004, Product_ID='P00184942', Gender='M', Age='46-50', Occupation=7, City_Category='B', Stay_In_Current_City_Years='2', Marital_Status=1, Product_Category_1=1, Product_Category_2=8, Product_Category_3=17, Purchase=19215),\n",
       " Row(User_ID=1000004, Product_ID='P00346142', Gender='M', Age='46-50', Occupation=7, City_Category='B', Stay_In_Current_City_Years='2', Marital_Status=1, Product_Category_1=1, Product_Category_2=15, Product_Category_3=None, Purchase=15854),\n",
       " Row(User_ID=1000004, Product_ID='P0097242', Gender='M', Age='46-50', Occupation=7, City_Category='B', Stay_In_Current_City_Years='2', Marital_Status=1, Product_Category_1=1, Product_Category_2=16, Product_Category_3=None, Purchase=15686),\n",
       " Row(User_ID=1000005, Product_ID='P00274942', Gender='M', Age='26-35', Occupation=20, City_Category='A', Stay_In_Current_City_Years='1', Marital_Status=1, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7871)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb6ffbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f072e",
   "metadata": {},
   "source": [
    "Impute Missing values\n",
    "\n",
    "We can check number of not null observations in train and test by calling drop() method. By default, drop() method will drop a row if it contains any null value. We can also pass ‘all” to drop a row only if all its values are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2baef100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166821, 71037)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.na.drop().count(),test.na.drop('any').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000018c1",
   "metadata": {},
   "source": [
    "Here, I am imputing null values in train and test file with -1. Imputing the values with -1 is not an elegant solution. We have several algorithms / techniques to impute null values, but for the simplicity I am imputing null with constant value (-1). We can transform our base train, test Dataframes after applying this imputation. For imputing constant value, we have fillna method. Lets fill the -1 in-place of null in all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d026796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf9f28",
   "metadata": {},
   "source": [
    "Analyzing numerical features\n",
    "\n",
    "We can also see the various summary Statistics of a Dataframe columns using describe() method, which shows statistics for numerical variables. To show the results we need to call show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7303922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+------------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|        Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+------------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|            550068|       550068|                    550068|             550068|            550068|            550068|            550068|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null| 8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 6.419769919355425| 3.145214773446192|9263.968712959126|\n",
      "| stddev| 1727.591585530871|      null|  null|  null|6.5226604873418115|         null|         0.989086680757309| 0.4917701263173259| 3.936211369201324| 6.565109781181374| 6.681038828257864|5023.065393820593|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                 0|            A|                         0|                  0|                 1|                -1|                -1|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|                20|            C|                        4+|                  1|                20|                18|                18|            23961|\n",
      "+-------+------------------+----------+------+------+------------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13998be4",
   "metadata": {},
   "source": [
    "Sub-setting Columns\n",
    "\n",
    "Let’s select a column called ‘User_ID’ from a train, we need to call a method ‘select’ and pass the column name which we want to select. The select method will show result for selected column. We can also select more than one column from a data frame by providing columns name separated by comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a04499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|User_ID|\n",
      "+-------+\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000001|\n",
      "|1000002|\n",
      "|1000003|\n",
      "|1000004|\n",
      "|1000004|\n",
      "|1000004|\n",
      "|1000005|\n",
      "|1000005|\n",
      "|1000005|\n",
      "|1000005|\n",
      "|1000005|\n",
      "|1000006|\n",
      "|1000006|\n",
      "|1000006|\n",
      "|1000006|\n",
      "|1000007|\n",
      "|1000008|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('User_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9dd76",
   "metadata": {},
   "source": [
    "Analyzing categorical features\n",
    "\n",
    "To start building a model, we need to see the distribution of categorical features in train and test. Here I am showing this for only Product_ID but we can also do the same for any categorical feature. Let’s see the number of distinct categories of “Product_ID” in train and test. Which we can do by applying methods distinct() and count()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b0473d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3631, 3491)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('Product_ID').distinct().count(), test.select('Product_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d9d54",
   "metadata": {},
   "source": [
    "After counting the number of distinct values for train and test we can see the train has more categories than test. Let us check what are the categories for Product_ID, which are in test but not in train by applying subtract method.We can also do the same for all categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1fce609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))\n",
    "diff_cat_in_train_test.distinct().count()# For distict count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a7c48",
   "metadata": {},
   "source": [
    "Above you can see that 46 different categories are in test not in train. In this case, either we collect more data about them or skip the rows in test for those categories(invalid category) which are not in train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fccd14",
   "metadata": {},
   "source": [
    "Transforming categorical variables to labels\n",
    "\n",
    "We also need to transform categorical columns to label by applying StringIndexer Transformation on Product_ID which will encode the Product_ID column of labels to a column of label indices. You can see more about this from the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "894c2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "plan_indexer = StringIndexer(inputCol = 'Product_ID', outputCol = 'product_ID')\n",
    "labeller = plan_indexer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24e9ce",
   "metadata": {},
   "source": [
    "Above, we build a ‘labeller’ by applying fit() method on train Dataframe. Later we will use this ‘labeller’ to transform our train and test. Let us transform our train and test Dataframe with the help of labeller. We need to call transform method for doing that. We will store the transformation result in Train1 and Test1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6dd3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train1 = labeller.transform(train)\n",
    "Test1 = labeller.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14f27b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|1000001|     765.0|     F| 0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|\n",
      "|1000001|     183.0|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|\n",
      "|1000001|    1496.0|     F| 0-17|        10|            A|                         2|             0|                12|                -1|                -1|    1422|\n",
      "|1000001|     480.0|     F| 0-17|        10|            A|                         2|             0|                12|                14|                -1|    1057|\n",
      "|1000002|     860.0|     M|  55+|        16|            C|                        4+|             0|                 8|                -1|                -1|    7969|\n",
      "|1000003|     157.0|     M|26-35|        15|            A|                         3|             0|                 1|                 2|                -1|   15227|\n",
      "|1000004|       5.0|     M|46-50|         7|            B|                         2|             1|                 1|                 8|                17|   19215|\n",
      "|1000004|     178.0|     M|46-50|         7|            B|                         2|             1|                 1|                15|                -1|   15854|\n",
      "|1000004|      51.0|     M|46-50|         7|            B|                         2|             1|                 1|                16|                -1|   15686|\n",
      "|1000005|      78.0|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    7871|\n",
      "|1000005|      27.0|     M|26-35|        20|            A|                         1|             1|                 5|                11|                -1|    5254|\n",
      "|1000005|     127.0|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    3957|\n",
      "|1000005|    2304.0|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    6073|\n",
      "|1000005|       9.0|     M|26-35|        20|            A|                         1|             1|                 1|                 2|                 5|   15665|\n",
      "|1000006|    1677.0|     F|51-55|         9|            A|                         1|             0|                 5|                 8|                14|    5378|\n",
      "|1000006|     929.0|     F|51-55|         9|            A|                         1|             0|                 4|                 5|                -1|    2079|\n",
      "|1000006|    1009.0|     F|51-55|         9|            A|                         1|             0|                 2|                 3|                 4|   13055|\n",
      "|1000006|     207.0|     F|51-55|         9|            A|                         1|             0|                 5|                14|                -1|    8851|\n",
      "|1000007|      58.0|     M|36-45|         1|            B|                         1|             1|                 1|                14|                16|   11788|\n",
      "|1000008|      81.0|     M|26-35|        12|            C|                        4+|             1|                 1|                 5|                15|   19614|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Train1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8de59",
   "metadata": {},
   "source": [
    "Selecting Features to Build a Machine Learning Model\n",
    "\n",
    "Let’s try to create a formula for Machine learning model like we do in R. First, we need to import RFormula from the pyspark.ml.feature. Then we need to specify the dependent and independent column inside this formula. We also have to specify the names for features column and label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddfdd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "formula = RFormula(formula=\"Purchase ~ Age+ Occupation +City_Category+Stay_In_Current_City_Years+Product_Category_1+Product_Category_2+ Gender\",featuresCol=\"features\",labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003c4e6",
   "metadata": {},
   "source": [
    "After creating the formula we need to fit this formula on our Train1 and transform Train1,Test1 through this formula. Lets see how to do this and after fitting transform train1,Test1 in train1,test1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9afbbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = formula.fit(Train1)\n",
    "train1 = t1.transform(Train1)\n",
    "test1 = t1.transform(Test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73e36ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+--------------------+-------+\n",
      "|User_ID|product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|            features|  label|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+--------------------+-------+\n",
      "|1000001|     765.0|     F| 0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|(16,[6,10,13,14],...| 8370.0|\n",
      "|1000001|     183.0|     F| 0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|(16,[6,10,13,14],...|15200.0|\n",
      "|1000001|    1496.0|     F| 0-17|        10|            A|                         2|             0|                12|                -1|                -1|    1422|(16,[6,10,13,14],...| 1422.0|\n",
      "|1000001|     480.0|     F| 0-17|        10|            A|                         2|             0|                12|                14|                -1|    1057|(16,[6,10,13,14],...| 1057.0|\n",
      "|1000002|     860.0|     M|  55+|        16|            C|                        4+|             0|                 8|                -1|                -1|    7969|(16,[5,6,8,12,13,...| 7969.0|\n",
      "|1000003|     157.0|     M|26-35|        15|            A|                         3|             0|                 1|                 2|                -1|   15227|(16,[0,6,11,13,14...|15227.0|\n",
      "|1000004|       5.0|     M|46-50|         7|            B|                         2|             1|                 1|                 8|                17|   19215|(16,[3,6,7,10,13,...|19215.0|\n",
      "|1000004|     178.0|     M|46-50|         7|            B|                         2|             1|                 1|                15|                -1|   15854|(16,[3,6,7,10,13,...|15854.0|\n",
      "|1000004|      51.0|     M|46-50|         7|            B|                         2|             1|                 1|                16|                -1|   15686|(16,[3,6,7,10,13,...|15686.0|\n",
      "|1000005|      78.0|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    7871|(16,[0,6,9,13,14,...| 7871.0|\n",
      "|1000005|      27.0|     M|26-35|        20|            A|                         1|             1|                 5|                11|                -1|    5254|(16,[0,6,9,13,14,...| 5254.0|\n",
      "|1000005|     127.0|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    3957|(16,[0,6,9,13,14,...| 3957.0|\n",
      "|1000005|    2304.0|     M|26-35|        20|            A|                         1|             1|                 8|                -1|                -1|    6073|(16,[0,6,9,13,14,...| 6073.0|\n",
      "|1000005|       9.0|     M|26-35|        20|            A|                         1|             1|                 1|                 2|                 5|   15665|(16,[0,6,9,13,14,...|15665.0|\n",
      "|1000006|    1677.0|     F|51-55|         9|            A|                         1|             0|                 5|                 8|                14|    5378|(16,[4,6,9,13,14]...| 5378.0|\n",
      "|1000006|     929.0|     F|51-55|         9|            A|                         1|             0|                 4|                 5|                -1|    2079|(16,[4,6,9,13,14]...| 2079.0|\n",
      "|1000006|    1009.0|     F|51-55|         9|            A|                         1|             0|                 2|                 3|                 4|   13055|(16,[4,6,9,13,14]...|13055.0|\n",
      "|1000006|     207.0|     F|51-55|         9|            A|                         1|             0|                 5|                14|                -1|    8851|(16,[4,6,9,13,14]...| 8851.0|\n",
      "|1000007|      58.0|     M|36-45|         1|            B|                         1|             1|                 1|                14|                16|   11788|(16,[1,6,7,9,13,1...|11788.0|\n",
      "|1000008|      81.0|     M|26-35|        12|            C|                        4+|             1|                 1|                 5|                15|   19614|(16,[0,6,8,12,13,...|19614.0|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0306d3",
   "metadata": {},
   "source": [
    "After applying the formula we can see that train1 and test1 have 2 extra columns called features and label those we have specified in the formula (featuresCol=”features” and labelCol=”label”). The intuition is that all categorical variables in the features column in train1 and test1 are transformed to the numerical and the numerical variables are same as before for applying ML. Purchase variable will transom to label column. We can also look at the column features and label in train1 and test1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0855c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(16,[6,10,13,14],...|\n",
      "|(16,[6,10,13,14],...|\n",
      "|(16,[6,10,13,14],...|\n",
      "|(16,[6,10,13,14],...|\n",
      "|(16,[5,6,8,12,13,...|\n",
      "|(16,[0,6,11,13,14...|\n",
      "|(16,[3,6,7,10,13,...|\n",
      "|(16,[3,6,7,10,13,...|\n",
      "|(16,[3,6,7,10,13,...|\n",
      "|(16,[0,6,9,13,14,...|\n",
      "|(16,[0,6,9,13,14,...|\n",
      "|(16,[0,6,9,13,14,...|\n",
      "|(16,[0,6,9,13,14,...|\n",
      "|(16,[0,6,9,13,14,...|\n",
      "|(16,[4,6,9,13,14]...|\n",
      "|(16,[4,6,9,13,14]...|\n",
      "|(16,[4,6,9,13,14]...|\n",
      "|(16,[4,6,9,13,14]...|\n",
      "|(16,[1,6,7,9,13,1...|\n",
      "|(16,[0,6,8,12,13,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+\n",
      "|  label|\n",
      "+-------+\n",
      "| 8370.0|\n",
      "|15200.0|\n",
      "| 1422.0|\n",
      "| 1057.0|\n",
      "| 7969.0|\n",
      "|15227.0|\n",
      "|19215.0|\n",
      "|15854.0|\n",
      "|15686.0|\n",
      "| 7871.0|\n",
      "| 5254.0|\n",
      "| 3957.0|\n",
      "| 6073.0|\n",
      "|15665.0|\n",
      "| 5378.0|\n",
      "| 2079.0|\n",
      "|13055.0|\n",
      "| 8851.0|\n",
      "|11788.0|\n",
      "|19614.0|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train1.select('features').show()\n",
    "train1.select('label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd36b18",
   "metadata": {},
   "source": [
    "Building a Machine Learning Model: Random Forest\n",
    "\n",
    "After applying the RFormula and transforming the Dataframe, we now need to develop the machine learning model on this data. I want to apply a random forest regressor for this task. Let us import a random forest regressor, which is defined in pyspark.ml.regression and then create a model called rf. I am going to use default parameters for randomforest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c5eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23387e",
   "metadata": {},
   "source": [
    "After creating a model rf we need to divide our train1 data to train_cv and test_cv for cross validation.\n",
    "\n",
    "Here we are dividing train1 Dataframe in 70% for train_cv and 30% test_cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "531ee899",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_cv, test_cv) = train1.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04af9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build the model on train_cv and predict on test_cv. The results will save in  predictions.\n",
    "model1 = rf.fit(train_cv)\n",
    "predictions = model1.transform(test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59f6c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+--------------------+-------+------------------+\n",
      "|User_ID|product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|            features|  label|        prediction|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+--------------------+-------+------------------+\n",
      "|1000001|       1.0|     F|0-17|        10|            A|                         2|             0|                 1|                 2|                 9|   15416|(16,[6,10,13,14],...|15416.0|13106.263531426179|\n",
      "|1000001|      16.0|     F|0-17|        10|            A|                         2|             0|                 1|                 2|                 5|   11769|(16,[6,10,13,14],...|11769.0|13106.263531426179|\n",
      "|1000001|      19.0|     F|0-17|        10|            A|                         2|             0|                 4|                 8|                 9|    2763|(16,[6,10,13,14],...| 2763.0| 6832.407347140237|\n",
      "|1000001|      29.0|     F|0-17|        10|            A|                         2|             0|                 3|                 4|                 5|   13650|(16,[6,10,13,14],...|13650.0| 9434.111558684726|\n",
      "|1000001|     484.0|     F|0-17|        10|            A|                         2|             0|                 3|                 4|                -1|   11039|(16,[6,10,13,14],...|11039.0| 9434.111558684726|\n",
      "|1000001|     503.0|     F|0-17|        10|            A|                         2|             0|                 3|                 4|                12|   13627|(16,[6,10,13,14],...|13627.0| 9434.111558684726|\n",
      "|1000001|     892.0|     F|0-17|        10|            A|                         2|             0|                 3|                 4|                12|   10572|(16,[6,10,13,14],...|10572.0| 9434.111558684726|\n",
      "|1000001|     935.0|     F|0-17|        10|            A|                         2|             0|                 4|                 8|                -1|    2849|(16,[6,10,13,14],...| 2849.0| 6832.407347140237|\n",
      "|1000002|       2.0|     M| 55+|        16|            C|                        4+|             0|                 1|                 2|                 8|   19033|(16,[5,6,8,12,13,...|19033.0|13866.216005702907|\n",
      "|1000002|       3.0|     M| 55+|        16|            C|                        4+|             0|                 1|                 2|                14|   15388|(16,[5,6,8,12,13,...|15388.0|13866.216005702907|\n",
      "|1000002|      22.0|     M| 55+|        16|            C|                        4+|             0|                 5|                14|                17|    8644|(16,[5,6,8,12,13,...| 8644.0| 7634.475879447535|\n",
      "|1000002|      32.0|     M| 55+|        16|            C|                        4+|             0|                 1|                16|                -1|   15669|(16,[5,6,8,12,13,...|15669.0|13393.871536444241|\n",
      "|1000002|      37.0|     M| 55+|        16|            C|                        4+|             0|                 2|                 5|                 8|   15870|(16,[5,6,8,12,13,...|15870.0|11793.989083293662|\n",
      "|1000002|      49.0|     M| 55+|        16|            C|                        4+|             0|                 6|                 8|                -1|    8471|(16,[5,6,8,12,13,...| 8471.0|10392.315625196085|\n",
      "|1000002|      74.0|     M| 55+|        16|            C|                        4+|             0|                 8|                15|                -1|   10074|(16,[5,6,8,12,13,...|10074.0| 8309.342398118608|\n",
      "|1000002|      76.0|     M| 55+|        16|            C|                        4+|             0|                 1|                15|                -1|   15468|(16,[5,6,8,12,13,...|15468.0|13393.871536444241|\n",
      "|1000002|     130.0|     M| 55+|        16|            C|                        4+|             0|                 8|                16|                -1|   10074|(16,[5,6,8,12,13,...|10074.0| 8287.732016954238|\n",
      "|1000002|     175.0|     M| 55+|        16|            C|                        4+|             0|                 1|                16|                -1|    7927|(16,[5,6,8,12,13,...| 7927.0|13393.871536444241|\n",
      "|1000002|     215.0|     M| 55+|        16|            C|                        4+|             0|                 6|                 8|                -1|   20434|(16,[5,6,8,12,13,...|20434.0|10392.315625196085|\n",
      "|1000002|     223.0|     M| 55+|        16|            C|                        4+|             0|                 1|                 2|                15|   11577|(16,[5,6,8,12,13,...|11577.0|13866.216005702907|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+--------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you check the columns in predictions Dataframe, there is one column called prediction which has prediction result for test_cv.\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d68cd1",
   "metadata": {},
   "source": [
    "Lets evaluate our predictions on test_cv and see what is the mean squae error.\n",
    "\n",
    "To evaluate model we need to import RegressionEvaluator from the pyspark.ml.evaluation. We have to create an object for this. There is a method called evaluate for evaluator which will evaluate the model. We need to specify the metrics for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10134297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3871.8447318281524, 14991181.627385417)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\n",
    "mse = evaluator.evaluate(predictions,{evaluator.metricName:\"mse\" })\n",
    "import numpy as np\n",
    "np.sqrt(mse), mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa046113",
   "metadata": {},
   "source": [
    "After evaluation we can see that our root mean square error is 3773.1460883883865 which is a square root of mse.\n",
    "\n",
    "Now, we will implement the same process on full train1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33adb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf.fit(train1)\n",
    "predictions1 = model.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf59096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After prediction, we need to select those columns which are required in Black Friday competition submission.\n",
    "df = predictions1.selectExpr(\"User_ID as User_ID\", \"Product_ID as Product_ID\", \"prediction as Purchase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b673cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+\n",
      "|User_ID|Product_ID|          Purchase|\n",
      "+-------+----------+------------------+\n",
      "|1000004|      47.0|13343.564238703912|\n",
      "|1000009|     683.0| 7571.386861104364|\n",
      "|1000010|    1188.0| 6810.857837067696|\n",
      "|1000010|    2866.0| 6810.857837067696|\n",
      "|1000011|     193.0| 6879.479569910332|\n",
      "|1000013|     574.0| 11248.97582695539|\n",
      "|1000013|     334.0|12797.228817566061|\n",
      "|1000013|    1008.0| 11248.97582695539|\n",
      "|1000015|    1305.0| 8084.808476040179|\n",
      "|1000022|     420.0| 6891.603473330273|\n",
      "|1000026|       6.0|13295.424160294784|\n",
      "|1000026|    1154.0| 6942.365281084855|\n",
      "|1000026|     426.0| 6725.995605055779|\n",
      "|1000026|     606.0| 6725.995605055779|\n",
      "|1000028|     262.0| 8156.773606331614|\n",
      "|1000029|     312.0|   12685.222152834|\n",
      "|1000033|     885.0| 9280.889205648358|\n",
      "|1000033|     598.0| 7126.924556223976|\n",
      "|1000034|       0.0| 6952.060731188198|\n",
      "|1000035|    1009.0| 11248.97582695539|\n",
      "+-------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o753.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 126.0 failed 1 times, most recent failure: Lost task 0.0 in stage 126.0 (TID 108) (DESKTOP-8RP3DQ1 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3703/1356671337: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2620/438528713.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2617/409680661.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1217/1210294899.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Unseen label: P00168242. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel$$Lambda$3745/1089227265.apply(Unknown Source)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4336/671345382.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4334/324836616.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$923/333142902.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:394)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2761/1839773711.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2190/1467986812.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2195/669857496.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2191/353818532.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3703/1356671337: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2620/438528713.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2617/409680661.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1217/1210294899.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen label: P00168242. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel$$Lambda$3745/1089227265.apply(Unknown Source)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27632/1797867127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Now we need to write the df in csv format for submission.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df.toPandas().to_csv('submission.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mpdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    691\u001b[0m         \"\"\"\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o753.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 126.0 failed 1 times, most recent failure: Lost task 0.0 in stage 126.0 (TID 108) (DESKTOP-8RP3DQ1 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3703/1356671337: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2620/438528713.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2617/409680661.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1217/1210294899.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Unseen label: P00168242. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel$$Lambda$3745/1089227265.apply(Unknown Source)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4336/671345382.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4334/324836616.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$923/333142902.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:394)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2761/1839773711.apply(Unknown Source)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.Dataset$$Lambda$2190/1467986812.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2195/669857496.apply(Unknown Source)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2191/353818532.apply(Unknown Source)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$3703/1356671337: (string) => double)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2620/438528713.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$2617/409680661.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1217/1210294899.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen label: P00168242. To handle unseen labels, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel$$Lambda$3745/1089227265.apply(Unknown Source)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "# Now we need to write the df in csv format for submission.\n",
    "df.show()\n",
    "# Not making the jump to python\n",
    "df.toPandas().show()\n",
    "\n",
    "#df.toPandas().to_csv('submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7411290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9ddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
